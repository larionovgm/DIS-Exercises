{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqClh2tJcRms"
   },
   "source": [
    "# Exercise 2: Indexing and Probabilistic Retrieval\n",
    "The following code is modified from Exercise 1. It is used to construct the vocabulary and vectorize the documents and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NFhRavg7cRmt",
    "outputId": "8a0208cc-d702-4e0b-a001-a2032ddb6efd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/george/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/george/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPyiybsjcRmx"
   },
   "source": [
    "## Question 1 -  Distributed Information Retrieval\n",
    "\n",
    "In this exercise we implement a core process of Fagin's algorithm, the parallel scanning of the posting lists. Assume an aggregation function that returns the sum of the tf-idf scores given the terms in a document.\n",
    "\n",
    "We assume that the posting lists for each term of a retrieval system are running on a different node.\n",
    "\n",
    "We first create a dictionary $h$, where each entry of the dictionary corresponds to a posting list for a term, assumed to be hosted in a different node. Explore the structure of $h$, to understand it. We suggest to first use the simple collection breads.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPvJvqxWcRmy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "doc_vecs = np.transpose(np.array(document_vectors))\n",
    "h = {}\n",
    "for i, term in enumerate(vocabulary):\n",
    "    ha = {}\n",
    "    for docj in range(len(original_documents)):\n",
    "        tfidf = doc_vecs[i][docj]\n",
    "        ha[docj] = tfidf\n",
    "    sorted_ha = sorted(ha.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    h[term] = sorted_ha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes\n",
      "Smith Pies: Best Pies in London\n",
      "Numerical Recipes: The Art of Scientific Computing\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Pastry: A Book of Best French Pastry Recipes\n",
      "art [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)]\n",
      "bake [(0, 0.9162907318741551), (3, 0.9162907318741551), (1, 0.0), (2, 0.0), (4, 0.0)]\n",
      "best [(1, 0.45814536593707755), (4, 0.45814536593707755), (0, 0.0), (2, 0.0), (3, 0.0)]\n",
      "book [(4, 0.8047189562170501), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0)]\n",
      "bread [(3, 0.9162907318741551), (0, 0.45814536593707755), (1, 0.0), (2, 0.0), (4, 0.0)]\n",
      "cake [(3, 1.6094379124341003), (0, 0.0), (1, 0.0), (2, 0.0), (4, 0.0)]\n",
      "comput [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)]\n",
      "french [(4, 0.8047189562170501), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0)]\n",
      "london [(1, 0.8047189562170501), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]\n",
      "numer [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)]\n",
      "pastri [(3, 0.9162907318741551), (4, 0.9162907318741551), (0, 0.0), (1, 0.0), (2, 0.0)]\n",
      "pie [(1, 0.9162907318741551), (3, 0.9162907318741551), (0, 0.0), (2, 0.0), (4, 0.0)]\n",
      "quantiti [(3, 1.6094379124341003), (0, 0.0), (1, 0.0), (2, 0.0), (4, 0.0)]\n",
      "recip [(2, 0.22314355131420976), (3, 0.22314355131420976), (0, 0.11157177565710488), (4, 0.11157177565710488), (1, 0.0)]\n",
      "scientif [(2, 1.6094379124341003), (0, 0.0), (1, 0.0), (3, 0.0), (4, 0.0)]\n",
      "smith [(1, 0.8047189562170501), (0, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]\n",
      "without [(0, 0.8047189562170501), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "for document in original_documents:\n",
    "    print(document)\n",
    "for i, term in enumerate(vocabulary):\n",
    "    print(term, h[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art [0.         0.         1.60943791 0.         0.        ]\n",
      "bake [0.91629073 0.         0.         0.91629073 0.        ]\n",
      "best [0.         0.45814537 0.         0.         0.45814537]\n",
      "book [0.         0.         0.         0.         0.80471896]\n",
      "bread [0.45814537 0.         0.         0.91629073 0.        ]\n",
      "cake [0.         0.         0.         1.60943791 0.        ]\n",
      "comput [0.         0.         1.60943791 0.         0.        ]\n",
      "french [0.         0.         0.         0.         0.80471896]\n",
      "london [0.         0.80471896 0.         0.         0.        ]\n",
      "numer [0.         0.         1.60943791 0.         0.        ]\n",
      "pastri [0.         0.         0.         0.91629073 0.91629073]\n",
      "pie [0.         0.91629073 0.         0.91629073 0.        ]\n",
      "quantiti [0.         0.         0.         1.60943791 0.        ]\n",
      "recip [0.11157178 0.         0.22314355 0.22314355 0.11157178]\n",
      "scientif [0.         0.         1.60943791 0.         0.        ]\n",
      "smith [0.         0.80471896 0.         0.         0.        ]\n",
      "without [0.80471896 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "for i, term in enumerate(vocabulary):\n",
    "    print(term, doc_vecs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, k = \"bread recipe\", 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Split and stem the query\n",
    "    q = query.split()\n",
    "    q = set([stemmer.stem(w) for w in q])\n",
    "    query_term_cnt = len(q)\n",
    "    \n",
    "    # select the posting lists for the query terms\n",
    "    posting_lists = {}\n",
    "    for term in q:\n",
    "        if term in h:\n",
    "            posting_lists[term] = h[term]\n",
    "    \n",
    "    max_len = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recip': [(2, 0.22314355131420976), (3, 0.22314355131420976), (0, 0.11157177565710488), (4, 0.11157177565710488), (1, 0.0)], 'bread': [(3, 0.9162907318741551), (0, 0.45814536593707755), (1, 0.0), (2, 0.0), (4, 0.0)]}\n",
      "5\n",
      "(2, 0.22314355131420976)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(posting_lists)\n",
    "print(max_len)\n",
    "print(posting_lists['recip'][0])\n",
    "print(query_term_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54S-YarVcRm1"
   },
   "source": [
    "### Complete the following function that implements the Fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5UoBYCJYcRm2"
   },
   "outputs": [],
   "source": [
    "def fagin_algorithm(query, h, k, vocabulary):\n",
    "    \n",
    "    # Split and stem the query\n",
    "    q = query.split()\n",
    "    q = set([stemmer.stem(w) for w in q])\n",
    "    query_term_cnt = len(q)\n",
    "    \n",
    "    # select the posting lists for the query terms\n",
    "    posting_lists = {}\n",
    "    for term in q:\n",
    "        if term in h:\n",
    "            posting_lists[term] = h[term]\n",
    "    \n",
    "    max_len = len(documents)\n",
    "    \n",
    "    # Traverse the selected posting lists until we found k documents that appear in ALL posting lists\n",
    "    # This corresponds to phase 1 of Fagin's algorithm.\n",
    "    # As a result you produce a dictionary documents_occurrences, with the document identifiers as keys, \n",
    "    # and the number of documents found as value.\n",
    "    # We stop traversing the posting lists until we have found k documents that appear in ALL posting lists \n",
    "    # of the query terms\n",
    "    \n",
    "    documents_occurrences = {}\n",
    "\n",
    "    while k > 0:\n",
    "        for posting_list in posting_lists:\n",
    "            for term in range(query_term_cnt):\n",
    "                \n",
    "            \n",
    "    \n",
    "    print(documents_occurrences)\n",
    "        \n",
    "    # Retrieve for the found documents the tfidf values and add them up.\n",
    "    # For simplicity, we do not distinguish the cases whether the values have already been retrieved in the \n",
    "    # previous phase, or whether we use random access to obtain those values\n",
    "    \n",
    "    tfidf = {}\n",
    "    for d in documents_occurrences.keys():\n",
    "        t = 0\n",
    "        for term in q:\n",
    "            t = t + dict(posting_lists[term])[d]\n",
    "        tfidf[d] = t\n",
    "        \n",
    "    # Finally we compute the top-k documents and return the answer\n",
    "    \n",
    "    ans = sorted(tfidf.items(), key=lambda x:x[1], reverse = True)[:k]\n",
    "    return ans\n",
    "\n",
    "\n",
    "ans = fagin_algorithm(\"bread recipe\", h, 2, vocabulary)\n",
    "print(ans)\n",
    "for document_id in ans:\n",
    "    print(original_documents[document_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUA4odrOcRm5"
   },
   "source": [
    "Produce your own datasets to explore the behavior of the algorithm. Create a dataset such that for retrieving a 2 term query a total of 6 documents needs to be retrieved in the first phase of the algorithm (as in the example in the lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWJoWL2fcRm6"
   },
   "source": [
    "## Question 2: Probabilistic Retrieval\n",
    "\n",
    "Following the notation used in class, let us denote the set of terms by $T=\\{k_i|i=1,...,m\\}$, the set of documents by $D=\\{d_j |j=1,...,n\\}$, and let $d_i=(w_{1j},w_{2j},...,w_{mj})$. We are also given a query  $q=(w_{1q},w_{2q},...,w_{mq})$. In the lecture we studied that, \n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} \\frac{w_{ij}}{|d_j|}\\frac{w_{iq}}{|q|}$ .  (1)\n",
    "\n",
    "Another way of looking at the information retrieval problem is using a probabilistic approach. The probabilistic view of information retrieval consists of determining the conditional probability $P(q|d_j)$ that for a given document $d_j$ the query by the user is $q$. So, practically in probabilistic retrieval when a query $q$ is given, for each document it is evaluated how probable it is that the query is indeed relevant for the document, which results in a ranking of the documents.\n",
    "\n",
    "In order to relate vector space retrieval to a probabilistic view of information retrieval, we interpret the weights in Equation (1) as follows:\n",
    "\n",
    "-  $w_{ij}/|d_j|$ can be interpreted as the conditional probability $P(k_i|d_j)$ that for a given document $d_j$ the term $k_i$ is important (to characterize the document $d_j$).\n",
    "\n",
    "-  $w_{iq}/|q|$ can be interpreted as the conditional probability $P(q|k_i)$ that for a given term $k_i$ the query posed by the user is $q$. Intuitively, $P(q|k_i)$ gives the amount of importance given to a particular term while querying.\n",
    "\n",
    "With this interpretation you can rewrite Equation (1) as follows:\n",
    "\n",
    "$sim(q,d_j) = \\sum^m_{i=1} P(k_i|d_j)P(q|k_i)$ (2)\n",
    "\n",
    "### Question a\n",
    "Show that indeed with the probabilistic interpretation of weights of vector space retrieval, as given in Equation (2), the similarity computation in vector space retrieval results exactly in the probabilistic interpretation of information retrieval, i.e., $sim(q,d_j)= P(q|d_j)$.\n",
    "Given that $d_j$ and $q$ are conditionally independent, i.e., $P(d_j \\cap q|ki) = P(d_j|k_i)P(q|k_i)$. You can assume existence of joint probability density functions wherever required. (Hint: You might need to use Bayes theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVbeW5-ccRm7"
   },
   "source": [
    "### Question b\n",
    "Using the expression derived for $P(q|d_j)$ in (a), obtain a ranking (documents sorted in descending order of their scores) for the documents $P(k_i|d_1) = (0, 1/3, 2/3)$, $P(k_i|d_2) =(1/3, 2/3, 0)$, $P(k_i|d_3) = (1/2, 0, 1/2)$, and $P (k_i|d_4) = (3/4, 1/4, 0)$ and the query $P(q|k_i) = (1/5, 0, 2/3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7c4DATEbcRm8"
   },
   "source": [
    "### Question c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-t14nV4scRm9"
   },
   "source": [
    " Note that the model described in a. provides a probabilistic interpretation for vector space retrieval where weights are interpreted as probabilities . Compare to the probabilistic retrieval model based on language models introduced in the lecture and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7IEHaoaicRm-"
   },
   "source": [
    "## Question 3 - Inverted Files\n",
    "\n",
    "We learnt in the lecture that terms are typically stored in an inverted list. Now, in the inverted list, instead of only storing document identifiers of the documents in which the term appears, assume we also store an *offset* of the appearance of a term in a document. An $offset$ of a term $l_k$ given a document is defined as the number of words between the start of the document and $l_k$. Thus our inverted list is now:\n",
    "\n",
    "$l_k= \\langle f_k: \\{d_{i_1} \\rightarrow [o_1,\\ldots,o_{n_{i_1}}]\\}, \n",
    "\\{d_{i_2} \\rightarrow [o_1,\\ldots,o_{n_{i_2}}]\\}, \\ldots, \n",
    "\\{d_{i_k} \\rightarrow [o_1,\\ldots,o_{n_{i_k}}]\\} \\rangle$\n",
    "\n",
    "This means that in document $d_{i_1}$ term $l_k$ appears $n_{i_1}$ times and at offset $[o_1,,o_{n_{i_1}}]$, where $[o_1,,o_{n_{i_1}}]$ are sorted in ascending order, these type of indices are also known as term-offset indices. An example of a term-offset index is as follows:\n",
    "\n",
    "**Obama** = $⟨4 : {1 → [3]},{2 → [6]},{3 → [2,17]},{4 → [1]}⟩$\n",
    "\n",
    "**Governor** = $⟨2 : {4 → [3]}, {7 → [14]}⟩$\n",
    "\n",
    "**Election** = $⟨4 : {1 → [1]},{2 → [1,21]},{3 → [3]},{5 → [16,22,51]}⟩$\n",
    "\n",
    "Which is to say that the term **Governor** appear in 2 documents. In document 4 at offset 3, in document 7 at offset 14. Now let us consider the *SLOP/x* operator in text retrieval. This operator has the syntax: *QueryTerm1 SLOP/x QueryTerm2* finds occurrences of *QueryTerm1* within $x$ (but not necessarily in that order) words of *QueryTerm2*, where $x$ is a positive integer argument ($x \\geq 1$). Thus $x = 1$ demands that *QueryTerm1* be adjacent to *QueryTerm2*.\n",
    "\n",
    "1. List the set of documents that satisfy the query **Obama** *SLOP/2* **Election**.\n",
    "2. List each set of values for which the query **Obama** *SLOP/x* **Election** has a different set of documents as answers (starting from $x = 1$). \n",
    "3. Consider the general procedure for \"merging\" two term-offset inverted lists for a given document, to determine where the document satisfies a *SLOP/x* clause (since in general there will be many offsets at which each term occurs in a document). Let $L$ denote the total number of occurrences of the two terms in the document. Assume we have a pointer to the list of occurrences of each term and can move the pointer along this list. As we do so we check whether we have a hit for $SLOP/x$ (i.e. the $SLOP/x$ clause is satisfied). Each move of either pointer counts as a step. Based on this assumption is there a general \"merging\" procedure to determine whether the document satisfies a $SLOP/x$ clause, for which the following is true? Justify your answer.\n",
    "\n",
    "    1. The merge can be accomplished in a number of steps linear in $L$ regardless of $x$, and we can ensure that each pointer moves only to the right (i.e. forward).\n",
    "    2. The merge can be accomplished in a number of steps linear in $L$, but a pointer may be forced to move to the left (i.e. backwards).\n",
    "    3. The merge can require $x \\times L$ steps in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67Z8njyccRm_"
   },
   "source": [
    "## Question 4: Create inverted files using MapReduce\n",
    "In this exercise, we want to create the inverted files using the MapReduce framework. Write the **map** and **reduce** function. Note that the code of **run_mapreduce** is just a simulation of the behavior of  map-reduce system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DVXQdsCMcRnA"
   },
   "outputs": [],
   "source": [
    "with open(\"epfldocs.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "docs_n_doc_ids = list(zip(range(len(documents)), documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_p9DKAPXcRnD"
   },
   "outputs": [],
   "source": [
    "# Take a pair (doc_id, content). Example: (0, ['how', 'to', 'bake', 'bread', 'without', 'bake', 'recip'])\n",
    "# Return: list of (word, doc_id). Example: ('bake', 0)\n",
    "# For simplicity, we assume each mapper handles a document\n",
    "def map_function(doc_id, content):\n",
    "    keyvals = []\n",
    "    for word in content:\n",
    "        # YOUR CODE HERE\n",
    "    return keyvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "j-SAH9PLcRnG"
   },
   "outputs": [],
   "source": [
    "# Take a list of pairs (word, doc_id). Example:  [('bake', 0), ('bake', 0), ('bake', 3)]\n",
    "# Return: (word, frequency, list of document ids). Example ('bake', 3, [0, 0, 3])\n",
    "# For simplicity, we assume each reducer handles a word\n",
    "def reduce_function(lst):\n",
    "    total = 0\n",
    "    word = lst[0][0]\n",
    "    doc_ids = []\n",
    "    for w, doc_id in lst:\n",
    "        assert(word==w) # Assume each reducer handles 1 word\n",
    "        # YOUR CODE HERE\n",
    "    return (word, total, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kqUN9lIHcRnK"
   },
   "outputs": [],
   "source": [
    "# We simulate the mapreduce framework by this function\n",
    "def run_mapreduce(docs_n_doc_ids):\n",
    "    # Map phase\n",
    "    key_values = []\n",
    "    for doc_id, doc_content in docs_n_doc_ids:\n",
    "        key_values.extend(map_function(doc_id, doc_content))\n",
    "    # Shuffle phase\n",
    "    values = set(map(lambda x:x[0], key_values))\n",
    "    grouped_key_values = [[y for y in key_values if y[0]==x] for x in values]\n",
    "    # Reduce phase\n",
    "    inverted_files = []\n",
    "    for grouped_key_value in grouped_key_values:\n",
    "        word, total, doc_ids = reduce_function(grouped_key_value)\n",
    "        inverted_files.append((word, total, doc_ids))\n",
    "    return inverted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bV3h1-vZcRnO"
   },
   "outputs": [],
   "source": [
    "run_mapreduce(docs_n_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s5W87mCiccTo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Indexing_Probabilistic_Retrieval.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
